{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/space/hotel/phit/miniconda3/envs/zalo/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from evaluate import *\n",
    "eng_dataset = load_from_disk(\"processed.hf\")\n",
    "\n",
    "base_model = \"Intel/neural-chat-7b-v3-1\"\n",
    "optim=\"paged_adamw_8bit\"\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    ")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "                base_model,\n",
    "                device_map=\"auto\",\n",
    "                trust_remote_code=True,\n",
    "                # quantization_config=bnb_config,\n",
    "                # use_safetensors=True,\n",
    "                cache_dir=\"./cache/\"\n",
    "            )\n",
    "# model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "\n",
    "peft_model_id = \"lora-newral-chat\"\n",
    "config = PeftConfig.from_pretrained(peft_model_id)\n",
    "model = PeftModel.from_pretrained(model, peft_model_id, device_map=\"auto\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model, trust_remote_code=True, cache_dir=\"./cache/\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "model.config.eos_token_id = tokenizer.eos_token_id\n",
    "model.config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "NUM_SHOTS = 3\n",
    "shots = get_shots(\n",
    "    examples=[dict(zip(eng_dataset[\"train\"][:NUM_SHOTS],t)) for t in zip(*eng_dataset[\"train\"][:NUM_SHOTS].values())], \n",
    "    n_shots=NUM_SHOTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_path=\"./data/math_test.json\"\n",
    "test_data = read_json(test_path)['data']\n",
    "test_df = pd.DataFrame(test_data)\n",
    "test_df.drop(columns=[\"question\", \"choices\"], inplace=True)\n",
    "\n",
    "def write_predict_file(all_result, test_df=test_df, output_path=\"jupyter_submission.csv\"):\n",
    "    result_df = pd.DataFrame(all_result)\n",
    "    result_df.to_csv(output_path, index=False)\n",
    "    \n",
    "def write_time_file(all_predicted_time, test_df=test_df, output_path=\"time_submission.csv\"):\n",
    "    test_df[\"time\"] = all_predicted_time\n",
    "    test_df.to_csv(output_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(model, tokenizer, test_data, shots=None):\n",
    "    all_predicted_time = []\n",
    "    all_result = []\n",
    "\n",
    "    for sample in tqdm(test_data):\n",
    "        t1 = time()\n",
    "        trans_sample = translate_choice(sample) \n",
    "        dialog = transformer_for_test(trans_sample) \n",
    "        \n",
    "        prompts = get_dialog_string(dialog, shots)\n",
    "        response = ask_model(prompts, model, tokenizer, max_length = 1500, temperature = 0.1, top_k = 50)[0]\n",
    "        dialog.append({\n",
    "            \"role\": \"assistant\",\n",
    "            \"content\": response\n",
    "        })\n",
    "        answer = get_result(trans_sample, dialog)\n",
    "        t2 = time()\n",
    "        \n",
    "        predicted_time = int(t2*1000 - t1*1000)\n",
    "        all_predicted_time.append(predicted_time)\n",
    "        all_result.append(answer)\n",
    "        \n",
    "    return all_result, all_predicted_time\n",
    "\n",
    "all_result, all_predicted_time = inference(model, tokenizer, test_data, shots)\n",
    "        \n",
    "write_predict_file(all_result)\n",
    "write_time_file(all_predicted_time)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "zalo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
