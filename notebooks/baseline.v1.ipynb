{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token will not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n",
      "Token is valid (permission: read).\n",
      "Your token has been saved to /space/hotel/phit/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "login(token=\"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\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/space/hotel/phit/miniconda3/envs/zalo/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['question', 'choices', 'explanation', 'answer', 'clean_choices', 'clean_answer', 'A', 'B', 'C', 'D', 'id'],\n",
       "        num_rows: 1080\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['question', 'choices', 'explanation', 'answer', 'clean_choices', 'clean_answer', 'A', 'B', 'C', 'D', 'id'],\n",
       "        num_rows: 120\n",
       "    })\n",
       "})"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from peft import get_peft_config, get_peft_model, PromptTuningInit, PromptTuningConfig, TaskType, PeftType\n",
    "import torch\n",
    "import os\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "from datasets import load_dataset, load_from_disk, Dataset\n",
    "import pandas as pd\n",
    "from transformers import ( \n",
    "                        AutoTokenizer,\n",
    "                        AutoConfig,\n",
    "                        AutoModelForCausalLM,\n",
    "                        AutoTokenizer,\n",
    "                        AutoModelForMultipleChoice,\n",
    "                        default_data_collator,\n",
    "                        get_linear_schedule_with_warmup)\n",
    "  \n",
    "model_path = \"vinai/PhoGPT-7B5-Instruct\" \n",
    "\n",
    "dataset = load_from_disk(\"processed_train.hf\")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 1080/1080 [00:00<00:00, 3532.20 examples/s]\n",
      "Map: 100%|██████████| 120/120 [00:00<00:00, 2892.94 examples/s]\n"
     ]
    }
   ],
   "source": [
    "def generate_prompt(example):\n",
    "    question, options = example[\"question\"], \"\\n\".join(example[\"choices\"])\n",
    "    context = example[\"explanation\"]\n",
    "#     example[\"question_full\"] = f\"\"\"\n",
    "# Trả lời câu hỏi sau bằng cách xuất ra các chữ cái A, B, C hoặc D \\\n",
    "# theo thứ tự từ có khả năng đúng nhất đến có khả năng đúng ít nhất.\\n\\n{question}\n",
    "# \\n{options}\n",
    "# {context}\n",
    "# \"\"\"[1:]\n",
    "    answer = re.findall(\"^[ABCD].\\s?(.+)\", example[\"answer\"])[0]\n",
    "    example[\"question_full\"] = f\"\"\"\n",
    "### Câu hỏi: {question} \\\n",
    "\\n### Lựa chọn:\\n{options}\\n### Trả lời:\"\"\"[1:]\n",
    "\n",
    "    example[\"answer_full\"] = f\"\"\" \\\n",
    "### Giải thích: {context} \\\n",
    "\\n### Đáp án cuối cùng là: {answer}\"\"\"\n",
    "    return example\n",
    "        \n",
    "dataset = dataset.map(\n",
    "    generate_prompt\n",
    ")\n",
    "# answers = [k.replace(\"_\", \" \") for k in dataset[\"train\"][\"answer\"]]\n",
    "# dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Câu hỏi: Hai đường chéo của hình thoi có đặc điểm: \n",
      "### Lựa chọn:\n",
      "A. bằng nhau\n",
      "B. vuông góc\n",
      "C. bằng nhau và vuông góc\n",
      "D. không có đặc điểm\n",
      "### Trả lời:\n"
     ]
    }
   ],
   "source": [
    "print(dataset[\"train\"][\"question_full\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'Hai đường chéo của hình thoi có đặc điểm:',\n",
       " 'choices': ['A. bằng nhau',\n",
       "  'B. vuông góc',\n",
       "  'C. bằng nhau và vuông góc',\n",
       "  'D. không có đặc điểm'],\n",
       " 'explanation': '',\n",
       " 'answer': 'B. vuông góc',\n",
       " 'clean_choices': ['bằng nhau',\n",
       "  'vuông góc',\n",
       "  'bằng nhau và vuông góc',\n",
       "  'không có đặc điểm'],\n",
       " 'clean_answer': 'B',\n",
       " 'A': 'bằng nhau',\n",
       " 'B': 'vuông góc',\n",
       " 'C': 'bằng nhau và vuông góc',\n",
       " 'D': 'không có đặc điểm',\n",
       " 'id': '573',\n",
       " 'question_full': '### Câu hỏi: Hai đường chéo của hình thoi có đặc điểm: \\n### Lựa chọn:\\nA. bằng nhau\\nB. vuông góc\\nC. bằng nhau và vuông góc\\nD. không có đặc điểm\\n### Trả lời:',\n",
       " 'answer_full': ' ### Giải thích:  \\n### Đáp án cuối cùng là: vuông góc'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset[\"train\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# config = AutoConfig.from_pretrained(model_path, trust_remote_code=True, cache_dir=\"./cache\")  \n",
    "# config.init_device = \"cuda\"\n",
    "# # config.attn_config['attn_impl'] = 'triton' # Enable if \"triton\" installed!\n",
    "  \n",
    "# model = AutoModelForCausalLM.from_pretrained(  \n",
    "#     model_path, config=config, torch_dtype=torch.bfloat16, trust_remote_code=True, cache_dir=\"./cache\"\n",
    "# )\n",
    "# # If your GPU does not support bfloat16:\n",
    "# # model = AutoModelForCausalLM.from_pretrained(model_path, config=config, torch_dtype=torch.float16, trust_remote_code=True)\n",
    "# model.eval()  \n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True, cache_dir=\"./cache\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loader = iter(range(0, len(dataset[\"train\"]) - 1))\n",
    "# def generate_output(question):\n",
    "#     input_ids = tokenizer(question, return_tensors=\"pt\")  \n",
    "    \n",
    "#     outputs = model.generate(  \n",
    "#         inputs=input_ids[\"input_ids\"].to(\"cuda\"),  \n",
    "#         attention_mask=input_ids[\"attention_mask\"].to(\"cuda\"),  \n",
    "#         do_sample=True,  \n",
    "#         temperature=1.0,  \n",
    "#         top_k=50,  \n",
    "#         top_p=0.9,  \n",
    "#         max_new_tokens=1024,  \n",
    "#         eos_token_id=tokenizer.eos_token_id,  \n",
    "#         pad_token_id=tokenizer.pad_token_id  \n",
    "#     )  \n",
    "#     response = tokenizer.batch_decode(outputs, skip_special_tokens=True)[0]  \n",
    "    \n",
    "#     return response.split(\"### Trả lời:\")[1]\n",
    "# # response = response.split(\"### Trả lời:\")[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# idx = next(loader)\n",
    "# question = dataset[\"train\"][\"question_full\"][idx]\n",
    "# ground_truth = dataset[\"train\"][\"answer\"][idx]\n",
    "# print(question, generate_output(question))\n",
    "# print(f\"### ground truth: \", ground_truth)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 1080/1080 [00:01<00:00, 558.71 examples/s]\n",
      "Map: 100%|██████████| 120/120 [00:00<00:00, 749.67 examples/s]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True, cache_dir=\"./cache\")  \n",
    "encoded_dataset = dataset.map(lambda examples: {\n",
    "    \"input_ids\" : tokenizer(examples['question_full'], \n",
    "                           truncation=True, padding='max_length', return_tensors=\"pt\")[\"input_ids\"],\n",
    "    \"attention_mask\": tokenizer(examples['question_full'], \n",
    "                           truncation=True, padding='max_length', return_tensors=\"pt\")[\"attention_mask\"],\n",
    "    \"labels\":  tokenizer(examples['answer_full'], \n",
    "                           truncation=True, padding='max_length', return_tensors=\"pt\")[\"input_ids\"]\n",
    "    }, remove_columns=dataset[\"train\"].column_names, batched=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/space/hotel/phit/.cache/huggingface/modules/transformers_modules/vinai/PhoGPT-7B5-Instruct/d1a5a418bf01d49e8bf1b5b737b8ef143a33d9fd/configuration_mpt.py:97: UserWarning: alibi is turned on, setting `learned_pos_emb` to `False.`\n",
      "  warnings.warn(f'alibi is turned on, setting `learned_pos_emb` to `False.`')\n",
      "The model weights are not tied. Please use the `tie_weights` method before using the `infer_auto_device` function.\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:12<00:00,  6.35s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 33,554,432 || all params: 7,505,321,984 || trainable%: 0.44707518307052024\n"
     ]
    }
   ],
   "source": [
    "from transformers import (AutoModelForCausalLM, \n",
    "                        AutoTokenizer, \n",
    "                        default_data_collator, \n",
    "                        get_linear_schedule_with_warmup)\n",
    "from peft import   (get_peft_config, \n",
    "                    get_peft_model, \n",
    "                    PromptTuningInit, \n",
    "                    PromptTuningConfig, \n",
    "                    TaskType, \n",
    "                    PeftType, \n",
    "                    LoraConfig)\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "# os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:32\"\n",
    "\n",
    "device = \"cuda\"\n",
    "# peft_config = PromptTuningConfig(\n",
    "#     task_type=TaskType.CAUSAL_LM,\n",
    "#     prompt_tuning_init=PromptTuningInit.TEXT,\n",
    "#     num_virtual_tokens=8,\n",
    "#     prompt_tuning_init_text=\"\",\n",
    "#     tokenizer_name_or_path=model_path,\n",
    "#     # num_layers=3,\n",
    "#     # num_attention_heads\n",
    "# )\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.1,\n",
    "    r=64,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "dataset_name = \"mathsolving\"\n",
    "checkpoint_name = f\"{dataset_name}_{model_path}_{peft_config.peft_type}_{peft_config.task_type}_v1.pt\".replace(\n",
    "    \"/\", \"_\"\n",
    ")\n",
    "\n",
    "max_length = 64\n",
    "lr = 3e-2\n",
    "num_epochs = 5\n",
    "batch_size = 1\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    encoded_dataset[\"train\"], shuffle=True, collate_fn=default_data_collator, batch_size=batch_size, pin_memory=True\n",
    ")\n",
    "eval_dataloader = DataLoader(encoded_dataset[\"test\"], collate_fn=default_data_collator, batch_size=batch_size, pin_memory=True)\n",
    "# creating model\n",
    "\n",
    "config = AutoConfig.from_pretrained(model_path, trust_remote_code=True, \n",
    "                                    cache_dir=\"./cache\")  \n",
    "config.init_device = \"cuda\"\n",
    "# config.attn_config['attn_impl'] = 'triton' # Enable if \"triton\" installed!\n",
    "  \n",
    "# model = AutoModelForCausalLM.from_pretrained(  \n",
    "#     model_path, config=config, torch_dtype=torch.bfloat16, trust_remote_code=True, cache_dir=\"./cache\"\n",
    "# )\n",
    "\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path, config=config, device_map=\"auto\", \n",
    "                                             load_in_8bit=True, trust_remote_code=True, cache_dir=\"./cache\")\n",
    "\n",
    "for param in model.parameters():\n",
    "  param.requires_grad = False  # freeze the model - train adapters later\n",
    "  if param.ndim == 1:\n",
    "    # cast the small parameters (e.g. layernorm) to fp32 for stability\n",
    "    param.data = param.data.to(torch.float16)\n",
    "\n",
    "# model.gradient_checkpointing_enable()  # reduce number of stored activations\n",
    "model.enable_input_require_grads()\n",
    "\n",
    "# class CastOutputToFloat(nn.Sequential):\n",
    "#   def forward(self, x): return super().forward(x).to(torch.float32)\n",
    "# model.lm_head = CastOutputToFloat(model.lm_head)\n",
    "\n",
    "model = get_peft_model(model, peft_config)\n",
    "model.print_trainable_parameters()\n",
    "\n",
    "# model\n",
    "# optimizer and lr scheduler\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "lr_scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=(len(train_dataloader) * num_epochs),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1080/1080 [29:41<00:00,  1.65s/it]\n",
      "100%|██████████| 120/120 [01:23<00:00,  1.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=0: train_ppl=tensor(nan, device='cuda:0') train_epoch_loss=tensor(nan, device='cuda:0') eval_ppl=tensor(nan, device='cuda:0') eval_epoch_loss=tensor(nan, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1080/1080 [29:40<00:00,  1.65s/it]\n",
      "100%|██████████| 120/120 [01:23<00:00,  1.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=1: train_ppl=tensor(nan, device='cuda:0') train_epoch_loss=tensor(nan, device='cuda:0') eval_ppl=tensor(nan, device='cuda:0') eval_epoch_loss=tensor(nan, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1080/1080 [29:40<00:00,  1.65s/it]\n",
      "100%|██████████| 120/120 [01:23<00:00,  1.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=2: train_ppl=tensor(nan, device='cuda:0') train_epoch_loss=tensor(nan, device='cuda:0') eval_ppl=tensor(nan, device='cuda:0') eval_epoch_loss=tensor(nan, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1080/1080 [29:40<00:00,  1.65s/it]\n",
      "100%|██████████| 120/120 [01:23<00:00,  1.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=3: train_ppl=tensor(nan, device='cuda:0') train_epoch_loss=tensor(nan, device='cuda:0') eval_ppl=tensor(nan, device='cuda:0') eval_epoch_loss=tensor(nan, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1080/1080 [29:40<00:00,  1.65s/it]\n",
      "100%|██████████| 120/120 [01:23<00:00,  1.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=4: train_ppl=tensor(nan, device='cuda:0') train_epoch_loss=tensor(nan, device='cuda:0') eval_ppl=tensor(nan, device='cuda:0') eval_epoch_loss=tensor(nan, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# training and evaluation\n",
    "# model = model.to(\"cuda:0\")\n",
    "device = \"cuda:0\"\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for step, batch in enumerate(tqdm(train_dataloader)):\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        #         print(batch)\n",
    "        #         print(batch[\"input_ids\"].shape)\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        total_loss += loss.detach().float()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "    model.eval()\n",
    "    eval_loss = 0\n",
    "    eval_preds = []\n",
    "    for step, batch in enumerate(tqdm(eval_dataloader)):\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        eval_loss += loss.detach().float()\n",
    "        eval_preds.extend(\n",
    "            tokenizer.batch_decode(torch.argmax(outputs.logits, -1).detach().cpu().numpy(), skip_special_tokens=True)\n",
    "        )\n",
    "\n",
    "    eval_epoch_loss = eval_loss / len(eval_dataloader)\n",
    "    eval_ppl = torch.exp(eval_epoch_loss)\n",
    "    train_epoch_loss = total_loss / len(train_dataloader)\n",
    "    train_ppl = torch.exp(train_epoch_loss)\n",
    "    print(f\"{epoch=}: {train_ppl=} {train_epoch_loss=} {eval_ppl=} {eval_epoch_loss=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = iter(range(0, len(dataset[\"train\"]) - 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_output(question):\n",
    "    input_ids = tokenizer(question, return_tensors=\"pt\")  \n",
    "    \n",
    "    outputs = model.generate(  \n",
    "        inputs=input_ids[\"input_ids\"].to(\"cuda\"),  \n",
    "        attention_mask=input_ids[\"attention_mask\"].to(\"cuda\"),  \n",
    "        do_sample=True,  \n",
    "        temperature=1.0,  \n",
    "        top_k=50,  \n",
    "        top_p=0.9,  \n",
    "        max_new_tokens=1024,  \n",
    "        eos_token_id=tokenizer.eos_token_id,  \n",
    "        pad_token_id=tokenizer.pad_token_id  \n",
    "    )  \n",
    "    response = tokenizer.batch_decode(outputs, skip_special_tokens=True)[0]  \n",
    "    \n",
    "    return response.split(\"### Trả lời:\")[1]\n",
    "# response = response.split(\"### Trả lời:\")[1]\n",
    "\n",
    "idx = next(loader)\n",
    "question = dataset[\"train\"][\"question_full\"][idx]\n",
    "ground_truth = dataset[\"train\"][\"answer\"][idx]\n",
    "print(question, generate_output(question))\n",
    "print(f\"### ground truth: \", ground_truth)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/space/hotel/phit/miniconda3/envs/zalo/lib/python3.9/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "You're using a BloomTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "The model did not return a loss from the inputs, only the following keys: logits. For reference, the inputs it received are input_ids,attention_mask.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/space/hotel/phit/contest/zalo/ElementaryMathsSolving/baseline-1.ipynb Cell 28\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B172.16.87.76/space/hotel/phit/contest/zalo/ElementaryMathsSolving/baseline-1.ipynb#X36sdnNjb2RlLXJlbW90ZQ%3D%3D?line=17'>18</a>\u001b[0m trainer \u001b[39m=\u001b[39m Trainer(\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B172.16.87.76/space/hotel/phit/contest/zalo/ElementaryMathsSolving/baseline-1.ipynb#X36sdnNjb2RlLXJlbW90ZQ%3D%3D?line=18'>19</a>\u001b[0m     model\u001b[39m=\u001b[39mmodel, \u001b[39m# The model to train\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B172.16.87.76/space/hotel/phit/contest/zalo/ElementaryMathsSolving/baseline-1.ipynb#X36sdnNjb2RlLXJlbW90ZQ%3D%3D?line=19'>20</a>\u001b[0m     args\u001b[39m=\u001b[39mtraining_args, \u001b[39m# The training arguments\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B172.16.87.76/space/hotel/phit/contest/zalo/ElementaryMathsSolving/baseline-1.ipynb#X36sdnNjb2RlLXJlbW90ZQ%3D%3D?line=22'>23</a>\u001b[0m     tokenizer\u001b[39m=\u001b[39mtokenizer, \u001b[39m# The tokenizer\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B172.16.87.76/space/hotel/phit/contest/zalo/ElementaryMathsSolving/baseline-1.ipynb#X36sdnNjb2RlLXJlbW90ZQ%3D%3D?line=23'>24</a>\u001b[0m )\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B172.16.87.76/space/hotel/phit/contest/zalo/ElementaryMathsSolving/baseline-1.ipynb#X36sdnNjb2RlLXJlbW90ZQ%3D%3D?line=25'>26</a>\u001b[0m \u001b[39m# Start the training\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B172.16.87.76/space/hotel/phit/contest/zalo/ElementaryMathsSolving/baseline-1.ipynb#X36sdnNjb2RlLXJlbW90ZQ%3D%3D?line=26'>27</a>\u001b[0m trainer\u001b[39m.\u001b[39;49mtrain()\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B172.16.87.76/space/hotel/phit/contest/zalo/ElementaryMathsSolving/baseline-1.ipynb#X36sdnNjb2RlLXJlbW90ZQ%3D%3D?line=28'>29</a>\u001b[0m \u001b[39m# Evaluate the model\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B172.16.87.76/space/hotel/phit/contest/zalo/ElementaryMathsSolving/baseline-1.ipynb#X36sdnNjb2RlLXJlbW90ZQ%3D%3D?line=29'>30</a>\u001b[0m trainer\u001b[39m.\u001b[39mevaluate()\n",
      "File \u001b[0;32m/space/hotel/phit/miniconda3/envs/zalo/lib/python3.9/site-packages/transformers/trainer.py:1645\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1640\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel_wrapped \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\n\u001b[1;32m   1642\u001b[0m inner_training_loop \u001b[39m=\u001b[39m find_executable_batch_size(\n\u001b[1;32m   1643\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_inner_training_loop, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_train_batch_size, args\u001b[39m.\u001b[39mauto_find_batch_size\n\u001b[1;32m   1644\u001b[0m )\n\u001b[0;32m-> 1645\u001b[0m \u001b[39mreturn\u001b[39;00m inner_training_loop(\n\u001b[1;32m   1646\u001b[0m     args\u001b[39m=\u001b[39;49margs,\n\u001b[1;32m   1647\u001b[0m     resume_from_checkpoint\u001b[39m=\u001b[39;49mresume_from_checkpoint,\n\u001b[1;32m   1648\u001b[0m     trial\u001b[39m=\u001b[39;49mtrial,\n\u001b[1;32m   1649\u001b[0m     ignore_keys_for_eval\u001b[39m=\u001b[39;49mignore_keys_for_eval,\n\u001b[1;32m   1650\u001b[0m )\n",
      "File \u001b[0;32m/space/hotel/phit/miniconda3/envs/zalo/lib/python3.9/site-packages/transformers/trainer.py:1929\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1926\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontrol \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallback_handler\u001b[39m.\u001b[39mon_step_begin(args, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontrol)\n\u001b[1;32m   1928\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maccelerator\u001b[39m.\u001b[39maccumulate(model):\n\u001b[0;32m-> 1929\u001b[0m     tr_loss_step \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining_step(model, inputs)\n\u001b[1;32m   1931\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[1;32m   1932\u001b[0m     args\u001b[39m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   1933\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m is_torch_tpu_available()\n\u001b[1;32m   1934\u001b[0m     \u001b[39mand\u001b[39;00m (torch\u001b[39m.\u001b[39misnan(tr_loss_step) \u001b[39mor\u001b[39;00m torch\u001b[39m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   1935\u001b[0m ):\n\u001b[1;32m   1936\u001b[0m     \u001b[39m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   1937\u001b[0m     tr_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m tr_loss \u001b[39m/\u001b[39m (\u001b[39m1\u001b[39m \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mglobal_step \u001b[39m-\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[0;32m/space/hotel/phit/miniconda3/envs/zalo/lib/python3.9/site-packages/transformers/trainer.py:2750\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   2747\u001b[0m     \u001b[39mreturn\u001b[39;00m loss_mb\u001b[39m.\u001b[39mreduce_mean()\u001b[39m.\u001b[39mdetach()\u001b[39m.\u001b[39mto(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mdevice)\n\u001b[1;32m   2749\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcompute_loss_context_manager():\n\u001b[0;32m-> 2750\u001b[0m     loss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcompute_loss(model, inputs)\n\u001b[1;32m   2752\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mn_gpu \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m   2753\u001b[0m     loss \u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39mmean()  \u001b[39m# mean() to average on multi-gpu parallel training\u001b[39;00m\n",
      "File \u001b[0;32m/space/hotel/phit/miniconda3/envs/zalo/lib/python3.9/site-packages/transformers/trainer.py:2788\u001b[0m, in \u001b[0;36mTrainer.compute_loss\u001b[0;34m(self, model, inputs, return_outputs)\u001b[0m\n\u001b[1;32m   2786\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   2787\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(outputs, \u001b[39mdict\u001b[39m) \u001b[39mand\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mloss\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m outputs:\n\u001b[0;32m-> 2788\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m   2789\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mThe model did not return a loss from the inputs, only the following keys: \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   2790\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m,\u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin(outputs\u001b[39m.\u001b[39mkeys())\u001b[39m}\u001b[39;00m\u001b[39m. For reference, the inputs it received are \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m,\u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin(inputs\u001b[39m.\u001b[39mkeys())\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   2791\u001b[0m         )\n\u001b[1;32m   2792\u001b[0m     \u001b[39m# We don't use .loss here since the model may return tuples instead of ModelOutput.\u001b[39;00m\n\u001b[1;32m   2793\u001b[0m     loss \u001b[39m=\u001b[39m outputs[\u001b[39m\"\u001b[39m\u001b[39mloss\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(outputs, \u001b[39mdict\u001b[39m) \u001b[39melse\u001b[39;00m outputs[\u001b[39m0\u001b[39m]\n",
      "\u001b[0;31mValueError\u001b[0m: The model did not return a loss from the inputs, only the following keys: logits. For reference, the inputs it received are input_ids,attention_mask."
     ]
    }
   ],
   "source": [
    "# Import the Trainer and TrainingArguments classes\n",
    "from transformers import Trainer, TrainingArguments\n",
    "# torch.set_grad_enabled(True)\n",
    "# Create a TrainingArguments object\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"/space/hotel/phit/contest/zalo/Elementary Maths Solving/phoGPT-finetuned\", # The output directory\n",
    "    num_train_epochs=num_epochs, # The number of epochs\n",
    "    per_device_train_batch_size=1, # The batch size per device\n",
    "    per_device_eval_batch_size=1, # The batch size for evaluation\n",
    "    learning_rate=5e-5, # The initial learning rate\n",
    "    weight_decay=0.01, # The weight decay to apply\n",
    "    save_strategy=\"epoch\", # The checkpoint save strategy\n",
    "    half_precision_backend=\"auto\",\n",
    "    # gradient_checkpointing=True\n",
    ")\n",
    "\n",
    "# Create a Trainer object\n",
    "trainer = Trainer(\n",
    "    model=model, # The model to train\n",
    "    args=training_args, # The training arguments\n",
    "    train_dataset=encoded_dataset[\"train\"], # The training dataloader\n",
    "    eval_dataset=encoded_dataset[\"test\"], # The evaluation dataloader\n",
    "    tokenizer=tokenizer, # The tokenizer\n",
    "    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False)\n",
    ")\n",
    "\n",
    "# Start the training\n",
    "trainer.train()\n",
    "\n",
    "# Evaluate the model\n",
    "trainer.evaluate()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/space/hotel/phit/.cache/huggingface/modules/transformers_modules/vinai/PhoGPT-7B5-Instruct/d1a5a418bf01d49e8bf1b5b737b8ef143a33d9fd/configuration_mpt.py:97: UserWarning: alibi is turned on, setting `learned_pos_emb` to `False.`\n",
      "  warnings.warn(f'alibi is turned on, setting `learned_pos_emb` to `False.`')\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b412d59ea9a74b81bcd4647bb27333e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xformers is not installed correctly. If you want to use memory_efficient_attention to accelerate training use the following command to install Xformers\n",
      "pip install xformers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on 2 device...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/space/hotel/phit/miniconda3/envs/zalo/lib/python3.9/site-packages/composer/trainer/trainer.py:166: UserWarning: `device_train_microbatch_size='auto'` may potentially fail with unexpected CUDA errors. Auto microbatching attempts to catch CUDA Out of Memory errors and adjust the batch size, but it is possible CUDA will be put into an irrecoverable state due to PyTorch bugs, e.g. integer overflow. In this case, please manually set device_train_microbatch_size explicitly to an integer instead.\n",
      "  warnings.warn((\"`device_train_microbatch_size='auto'` may potentially fail with unexpected \"\n",
      "/space/hotel/phit/miniconda3/envs/zalo/lib/python3.9/site-packages/composer/trainer/trainer.py:982: UserWarning: No optimizer was specified. Defaulting to DecoupledSGDW(lr=0.1)\n",
      "  warnings.warn(('No optimizer was specified. Defaulting to '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "from transformers import TrainingArguments, AutoModelWithLMHead\n",
    "import transformers\n",
    "from composer import Trainer\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.distributed as dist\n",
    "from torchmetrics.classification import MulticlassAccuracy\n",
    "from composer.models.huggingface import HuggingFaceModel\n",
    "from composer.metrics import CrossEntropy\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") \n",
    "# Initialize distributed training\n",
    "# dist.init_process_group(backend='nccl')  # Use 'gloo' if you don't have NCCL\n",
    "\n",
    "\n",
    "data_collator = transformers.data.data_collator.default_data_collator\n",
    "train_dataloader = DataLoader(encoded_dataset[\"train\"], batch_size=1, shuffle=False, drop_last=False, collate_fn=data_collator)\n",
    "eval_dataloader = DataLoader(encoded_dataset[\"test\"],batch_size=1, shuffle=False, drop_last=False, collate_fn=data_collator)\n",
    "\n",
    "# model = AutoModelWithLMHead.from_pretrained(model_path)\n",
    "config = AutoConfig.from_pretrained(model_path, trust_remote_code=True, cache_dir=\"./cache\")  \n",
    "config.init_device = \"cuda\"\n",
    "# config.attn_config['attn_impl'] = 'triton' # Enable if \"triton\" installed!\n",
    "  \n",
    "model = AutoModelForCausalLM.from_pretrained(  \n",
    "    model_path, config=config, torch_dtype=torch.bfloat16, trust_remote_code=True, cache_dir=\"./cache\"\n",
    ")\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "if torch.cuda.device_count() > 1:\n",
    "    print(f\"Running on {torch.cuda.device_count()} device...\")\n",
    "    model = nn.DataParallel(model, device_ids=[0, 1])\n",
    "# model = nn.parallel.DistributedDataParallel(model)\n",
    "\n",
    "metrics = [CrossEntropy()]\n",
    "\n",
    "# Package as a trainer-friendly Composer model\n",
    "composer_model = HuggingFaceModel(model.module if isinstance(model, nn.DataParallel) else model, tokenizer=tokenizer, metrics=metrics, use_logits=True)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=composer_model,\n",
    "    max_duration=\"3ep\",\n",
    "    eval_interval=\"1ep\",\n",
    "    train_dataloader=train_dataloader,\n",
    "    eval_dataloader=eval_dataloader,\n",
    "    save_folder=\"./phoGPT-finetuned\",\n",
    "    save_latest_filename=\"latest.pt\",\n",
    "    save_overwrite=True,\n",
    "    save_interval=\"1ep\",\n",
    "    device_train_microbatch_size=\"auto\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "******************************\n",
      "Config:\n",
      "composer_commit_hash: None\n",
      "composer_version: 0.17.0\n",
      "node_name: unknown because NODENAME environment variable not set\n",
      "num_gpus_per_node: 1\n",
      "num_nodes: 1\n",
      "rank_zero_seed: 432761156\n",
      "\n",
      "******************************\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54abe4b6398a45aca1e2c836822ca0ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train          Epoch   0:    0%|| 0/1080 [00:00<?, ?ba/s]         "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. The train loop failed with an internal microbatch of size 1.The GPU does not have enough memory to process even 1 sample during train.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/space/hotel/phit/contest/zalo/Elementary Maths Solving/baseline.ipynb Cell 29\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B172.16.87.76/space/hotel/phit/contest/zalo/Elementary%20Maths%20Solving/baseline.ipynb#X43sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m trainer\u001b[39m.\u001b[39;49mfit()\n",
      "File \u001b[0;32m/space/hotel/phit/miniconda3/envs/zalo/lib/python3.9/site-packages/composer/trainer/trainer.py:1885\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[0;34m(self, train_dataloader, train_dataloader_label, train_subset_num_batches, spin_dataloaders, duration, reset_time, schedulers, scale_schedule_ratio, step_schedulers_every_batch, eval_dataloader, eval_subset_num_batches, eval_interval, device_train_microbatch_size, precision)\u001b[0m\n\u001b[1;32m   1882\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mscaler \u001b[39m=\u001b[39m ClosureGradScaler() \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_use_closures() \u001b[39melse\u001b[39;00m GradScaler()\n\u001b[1;32m   1884\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfirst_batch_complete \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m-> 1885\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_train_loop()\n",
      "File \u001b[0;32m/space/hotel/phit/miniconda3/envs/zalo/lib/python3.9/site-packages/composer/trainer/trainer.py:2060\u001b[0m, in \u001b[0;36mTrainer._train_loop\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2057\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlogger\u001b[39m.\u001b[39mlog_metrics({\u001b[39m'\u001b[39m\u001b[39mtime/token\u001b[39m\u001b[39m'\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mtimestamp\u001b[39m.\u001b[39mtoken\u001b[39m.\u001b[39mvalue})\n\u001b[1;32m   2058\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlogger\u001b[39m.\u001b[39mlog_metrics({\u001b[39m'\u001b[39m\u001b[39mtime/token_in_epoch\u001b[39m\u001b[39m'\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mtimestamp\u001b[39m.\u001b[39mtoken_in_epoch\u001b[39m.\u001b[39mvalue})\n\u001b[0;32m-> 2060\u001b[0m total_loss_dict \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_train_batch(use_grad_scaling)\n\u001b[1;32m   2062\u001b[0m \u001b[39mif\u001b[39;00m use_grad_scaling:\n\u001b[1;32m   2063\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mscaler\u001b[39m.\u001b[39mupdate()\n",
      "File \u001b[0;32m/space/hotel/phit/miniconda3/envs/zalo/lib/python3.9/site-packages/composer/trainer/trainer.py:2284\u001b[0m, in \u001b[0;36mTrainer._train_batch\u001b[0;34m(self, use_grad_scaling)\u001b[0m\n\u001b[1;32m   2282\u001b[0m     all_ranks_finished \u001b[39m=\u001b[39m all_ranks_finished_tensor\u001b[39m.\u001b[39mitem() \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m   2283\u001b[0m \u001b[39mif\u001b[39;00m found_cuda_oom \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m-> 2284\u001b[0m     _adjust_device_train_microbatch_size(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstate)\n\u001b[1;32m   2285\u001b[0m     \u001b[39m# Skip return and rerun after handling oom\u001b[39;00m\n\u001b[1;32m   2286\u001b[0m     \u001b[39mcontinue\u001b[39;00m\n",
      "File \u001b[0;32m/space/hotel/phit/miniconda3/envs/zalo/lib/python3.9/site-packages/composer/trainer/trainer.py:239\u001b[0m, in \u001b[0;36m_adjust_device_train_microbatch_size\u001b[0;34m(state)\u001b[0m\n\u001b[1;32m    237\u001b[0m \u001b[39massert\u001b[39;00m state\u001b[39m.\u001b[39mdevice_train_microbatch_size \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    238\u001b[0m \u001b[39mif\u001b[39;00m state\u001b[39m.\u001b[39mdevice_train_microbatch_size \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m--> 239\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m((\u001b[39m'\u001b[39m\u001b[39mCUDA out of memory. The train loop failed with an internal microbatch of size 1.\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m    240\u001b[0m                         \u001b[39m'\u001b[39m\u001b[39mThe GPU does not have enough memory to process even 1 sample during train.\u001b[39m\u001b[39m'\u001b[39m))\n\u001b[1;32m    241\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    242\u001b[0m     original_microbatch_size \u001b[39m=\u001b[39m state\u001b[39m.\u001b[39mdevice_train_microbatch_size\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. The train loop failed with an internal microbatch of size 1.The GPU does not have enough memory to process even 1 sample during train."
     ]
    }
   ],
   "source": [
    "trainer.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'AutoConfig' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/space/hotel/phit/contest/zalo/Elementary Maths Solving/baseline.ipynb Cell 28\u001b[0m line \u001b[0;36m5\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B172.16.87.76/space/hotel/phit/contest/zalo/Elementary%20Maths%20Solving/baseline.ipynb#X36sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtransformers\u001b[39;00m \u001b[39mimport\u001b[39;00m Trainer, TrainingArguments, AutoModelWithLMHead\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B172.16.87.76/space/hotel/phit/contest/zalo/Elementary%20Maths%20Solving/baseline.ipynb#X36sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39m# model = AutoModelWithLMHead.from_pretrained(model_path)\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B172.16.87.76/space/hotel/phit/contest/zalo/Elementary%20Maths%20Solving/baseline.ipynb#X36sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m config \u001b[39m=\u001b[39m AutoConfig\u001b[39m.\u001b[39mfrom_pretrained(model_path, trust_remote_code\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, cache_dir\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m./cache\u001b[39m\u001b[39m\"\u001b[39m)  \n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B172.16.87.76/space/hotel/phit/contest/zalo/Elementary%20Maths%20Solving/baseline.ipynb#X36sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m config\u001b[39m.\u001b[39minit_device \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mcuda\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B172.16.87.76/space/hotel/phit/contest/zalo/Elementary%20Maths%20Solving/baseline.ipynb#X36sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39m# config.attn_config['attn_impl'] = 'triton' # Enable if \"triton\" installed!\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'AutoConfig' is not defined"
     ]
    }
   ],
   "source": [
    "from torch import nn\n",
    "from transformers import Trainer, TrainingArguments, AutoModelWithLMHead\n",
    "\n",
    "# model = AutoModelWithLMHead.from_pretrained(model_path)\n",
    "config = AutoConfig.from_pretrained(model_path, trust_remote_code=True, cache_dir=\"./cache\")  \n",
    "config.init_device = \"cuda\"\n",
    "# config.attn_config['attn_impl'] = 'triton' # Enable if \"triton\" installed!\n",
    "  \n",
    "model = AutoModelForCausalLM.from_pretrained(  \n",
    "    model_path, config=config, torch_dtype=torch.bfloat16, trust_remote_code=True, cache_dir=\"./cache\"\n",
    ")\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./phoGPT-finetuned\", #The output directory\n",
    "    overwrite_output_dir=True, #overwrite the content of the output directory\n",
    "    num_train_epochs=3, # number of training epochs\n",
    "    per_device_train_batch_size=1, # batch size for training\n",
    "    per_device_eval_batch_size=2,  # batch size for evaluation\n",
    "    eval_steps = 400, # Number of update steps between two evaluations.\n",
    "    save_steps=800, # after # steps model is saved\n",
    "    warmup_steps=500,# number of warmup steps for learning rate scheduler\n",
    "    )\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=encoded_dataset[\"train\"],\n",
    "    eval_dataset=encoded_dataset[\"test\"],\n",
    "    device_train_microbatch_size=4,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Trainer' object has no attribute 'train'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/space/hotel/phit/contest/zalo/Elementary Maths Solving/baseline.ipynb Cell 29\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B172.16.87.76/space/hotel/phit/contest/zalo/Elementary%20Maths%20Solving/baseline.ipynb#X40sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m trainer\u001b[39m.\u001b[39;49mtrain()\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Trainer' object has no attribute 'train'"
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answers = dataset[\"train\"][\"answer\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/space/hotel/phit/miniconda3/envs/zalo/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.is_available()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "zaloai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
