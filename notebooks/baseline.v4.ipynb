{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/space/hotel/phit/miniconda3/envs/zalo/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['question', 'choices', 'explanation', 'answer', 'clean_choices', 'clean_answer', 'A', 'B', 'C', 'D', 'id'],\n",
       "        num_rows: 1080\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['question', 'choices', 'explanation', 'answer', 'clean_choices', 'clean_answer', 'A', 'B', 'C', 'D', 'id'],\n",
       "        num_rows: 120\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from peft import get_peft_config, get_peft_model, PromptTuningInit, PromptTuningConfig, TaskType, PeftType\n",
    "import torch\n",
    "import os\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "from datasets import load_dataset, load_from_disk, Dataset, concatenate_datasets\n",
    "import pandas as pd\n",
    "import re\n",
    "import random\n",
    "from transformers import ( \n",
    "                        Trainer,\n",
    "                        TrainingArguments,\n",
    "                        AutoTokenizer,\n",
    "                        AutoConfig,\n",
    "                        AutoModelForCausalLM,\n",
    "                        AutoModelForMultipleChoice,\n",
    "                        AutoModelForSeq2SeqLM,\n",
    "                        default_data_collator,\n",
    "                        get_linear_schedule_with_warmup)\n",
    "  \n",
    "# model_path = \"vinai/PhoGPT-7B5-Instruct\" \n",
    "\n",
    "eng_dataset = load_from_disk(\"processed.hf\")\n",
    "eng_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForMultipleChoice were not initialized from the model checkpoint at microsoft/deberta-v3-large and are newly initialized: ['pooler.dense.weight', 'pooler.dense.bias', 'classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/space/hotel/phit/miniconda3/envs/zalo/lib/python3.9/site-packages/transformers/convert_slow_tokenizer.py:473: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DebertaV2ForMultipleChoice(\n",
       "  (deberta): DebertaV2Model(\n",
       "    (embeddings): DebertaV2Embeddings(\n",
       "      (word_embeddings): Embedding(128100, 1024, padding_idx=0)\n",
       "      (LayerNorm): LayerNorm((1024,), eps=1e-07, elementwise_affine=True)\n",
       "      (dropout): StableDropout()\n",
       "    )\n",
       "    (encoder): DebertaV2Encoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): DebertaV2Layer(\n",
       "          (attention): DebertaV2Attention(\n",
       "            (self): DisentangledSelfAttention(\n",
       "              (query_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (pos_dropout): StableDropout()\n",
       "              (dropout): StableDropout()\n",
       "            )\n",
       "            (output): DebertaV2SelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-07, elementwise_affine=True)\n",
       "              (dropout): StableDropout()\n",
       "            )\n",
       "          )\n",
       "          (intermediate): DebertaV2Intermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): DebertaV2Output(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-07, elementwise_affine=True)\n",
       "            (dropout): StableDropout()\n",
       "          )\n",
       "        )\n",
       "        (1): DebertaV2Layer(\n",
       "          (attention): DebertaV2Attention(\n",
       "            (self): DisentangledSelfAttention(\n",
       "              (query_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (pos_dropout): StableDropout()\n",
       "              (dropout): StableDropout()\n",
       "            )\n",
       "            (output): DebertaV2SelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-07, elementwise_affine=True)\n",
       "              (dropout): StableDropout()\n",
       "            )\n",
       "          )\n",
       "          (intermediate): DebertaV2Intermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): DebertaV2Output(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-07, elementwise_affine=True)\n",
       "            (dropout): StableDropout()\n",
       "          )\n",
       "        )\n",
       "        (2): DebertaV2Layer(\n",
       "          (attention): DebertaV2Attention(\n",
       "            (self): DisentangledSelfAttention(\n",
       "              (query_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (pos_dropout): StableDropout()\n",
       "              (dropout): StableDropout()\n",
       "            )\n",
       "            (output): DebertaV2SelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-07, elementwise_affine=True)\n",
       "              (dropout): StableDropout()\n",
       "            )\n",
       "          )\n",
       "          (intermediate): DebertaV2Intermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): DebertaV2Output(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-07, elementwise_affine=True)\n",
       "            (dropout): StableDropout()\n",
       "          )\n",
       "        )\n",
       "        (3): DebertaV2Layer(\n",
       "          (attention): DebertaV2Attention(\n",
       "            (self): DisentangledSelfAttention(\n",
       "              (query_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (pos_dropout): StableDropout()\n",
       "              (dropout): StableDropout()\n",
       "            )\n",
       "            (output): DebertaV2SelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-07, elementwise_affine=True)\n",
       "              (dropout): StableDropout()\n",
       "            )\n",
       "          )\n",
       "          (intermediate): DebertaV2Intermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): DebertaV2Output(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-07, elementwise_affine=True)\n",
       "            (dropout): StableDropout()\n",
       "          )\n",
       "        )\n",
       "        (4): DebertaV2Layer(\n",
       "          (attention): DebertaV2Attention(\n",
       "            (self): DisentangledSelfAttention(\n",
       "              (query_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (pos_dropout): StableDropout()\n",
       "              (dropout): StableDropout()\n",
       "            )\n",
       "            (output): DebertaV2SelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-07, elementwise_affine=True)\n",
       "              (dropout): StableDropout()\n",
       "            )\n",
       "          )\n",
       "          (intermediate): DebertaV2Intermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): DebertaV2Output(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-07, elementwise_affine=True)\n",
       "            (dropout): StableDropout()\n",
       "          )\n",
       "        )\n",
       "        (5): DebertaV2Layer(\n",
       "          (attention): DebertaV2Attention(\n",
       "            (self): DisentangledSelfAttention(\n",
       "              (query_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (pos_dropout): StableDropout()\n",
       "              (dropout): StableDropout()\n",
       "            )\n",
       "            (output): DebertaV2SelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-07, elementwise_affine=True)\n",
       "              (dropout): StableDropout()\n",
       "            )\n",
       "          )\n",
       "          (intermediate): DebertaV2Intermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): DebertaV2Output(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-07, elementwise_affine=True)\n",
       "            (dropout): StableDropout()\n",
       "          )\n",
       "        )\n",
       "        (6): DebertaV2Layer(\n",
       "          (attention): DebertaV2Attention(\n",
       "            (self): DisentangledSelfAttention(\n",
       "              (query_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (pos_dropout): StableDropout()\n",
       "              (dropout): StableDropout()\n",
       "            )\n",
       "            (output): DebertaV2SelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-07, elementwise_affine=True)\n",
       "              (dropout): StableDropout()\n",
       "            )\n",
       "          )\n",
       "          (intermediate): DebertaV2Intermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): DebertaV2Output(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-07, elementwise_affine=True)\n",
       "            (dropout): StableDropout()\n",
       "          )\n",
       "        )\n",
       "        (7): DebertaV2Layer(\n",
       "          (attention): DebertaV2Attention(\n",
       "            (self): DisentangledSelfAttention(\n",
       "              (query_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (pos_dropout): StableDropout()\n",
       "              (dropout): StableDropout()\n",
       "            )\n",
       "            (output): DebertaV2SelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-07, elementwise_affine=True)\n",
       "              (dropout): StableDropout()\n",
       "            )\n",
       "          )\n",
       "          (intermediate): DebertaV2Intermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): DebertaV2Output(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-07, elementwise_affine=True)\n",
       "            (dropout): StableDropout()\n",
       "          )\n",
       "        )\n",
       "        (8): DebertaV2Layer(\n",
       "          (attention): DebertaV2Attention(\n",
       "            (self): DisentangledSelfAttention(\n",
       "              (query_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (pos_dropout): StableDropout()\n",
       "              (dropout): StableDropout()\n",
       "            )\n",
       "            (output): DebertaV2SelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-07, elementwise_affine=True)\n",
       "              (dropout): StableDropout()\n",
       "            )\n",
       "          )\n",
       "          (intermediate): DebertaV2Intermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): DebertaV2Output(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-07, elementwise_affine=True)\n",
       "            (dropout): StableDropout()\n",
       "          )\n",
       "        )\n",
       "        (9): DebertaV2Layer(\n",
       "          (attention): DebertaV2Attention(\n",
       "            (self): DisentangledSelfAttention(\n",
       "              (query_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (pos_dropout): StableDropout()\n",
       "              (dropout): StableDropout()\n",
       "            )\n",
       "            (output): DebertaV2SelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-07, elementwise_affine=True)\n",
       "              (dropout): StableDropout()\n",
       "            )\n",
       "          )\n",
       "          (intermediate): DebertaV2Intermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): DebertaV2Output(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-07, elementwise_affine=True)\n",
       "            (dropout): StableDropout()\n",
       "          )\n",
       "        )\n",
       "        (10): DebertaV2Layer(\n",
       "          (attention): DebertaV2Attention(\n",
       "            (self): DisentangledSelfAttention(\n",
       "              (query_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (pos_dropout): StableDropout()\n",
       "              (dropout): StableDropout()\n",
       "            )\n",
       "            (output): DebertaV2SelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-07, elementwise_affine=True)\n",
       "              (dropout): StableDropout()\n",
       "            )\n",
       "          )\n",
       "          (intermediate): DebertaV2Intermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): DebertaV2Output(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-07, elementwise_affine=True)\n",
       "            (dropout): StableDropout()\n",
       "          )\n",
       "        )\n",
       "        (11): DebertaV2Layer(\n",
       "          (attention): DebertaV2Attention(\n",
       "            (self): DisentangledSelfAttention(\n",
       "              (query_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (pos_dropout): StableDropout()\n",
       "              (dropout): StableDropout()\n",
       "            )\n",
       "            (output): DebertaV2SelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-07, elementwise_affine=True)\n",
       "              (dropout): StableDropout()\n",
       "            )\n",
       "          )\n",
       "          (intermediate): DebertaV2Intermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): DebertaV2Output(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-07, elementwise_affine=True)\n",
       "            (dropout): StableDropout()\n",
       "          )\n",
       "        )\n",
       "        (12): DebertaV2Layer(\n",
       "          (attention): DebertaV2Attention(\n",
       "            (self): DisentangledSelfAttention(\n",
       "              (query_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (pos_dropout): StableDropout()\n",
       "              (dropout): StableDropout()\n",
       "            )\n",
       "            (output): DebertaV2SelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-07, elementwise_affine=True)\n",
       "              (dropout): StableDropout()\n",
       "            )\n",
       "          )\n",
       "          (intermediate): DebertaV2Intermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): DebertaV2Output(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-07, elementwise_affine=True)\n",
       "            (dropout): StableDropout()\n",
       "          )\n",
       "        )\n",
       "        (13): DebertaV2Layer(\n",
       "          (attention): DebertaV2Attention(\n",
       "            (self): DisentangledSelfAttention(\n",
       "              (query_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (pos_dropout): StableDropout()\n",
       "              (dropout): StableDropout()\n",
       "            )\n",
       "            (output): DebertaV2SelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-07, elementwise_affine=True)\n",
       "              (dropout): StableDropout()\n",
       "            )\n",
       "          )\n",
       "          (intermediate): DebertaV2Intermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): DebertaV2Output(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-07, elementwise_affine=True)\n",
       "            (dropout): StableDropout()\n",
       "          )\n",
       "        )\n",
       "        (14): DebertaV2Layer(\n",
       "          (attention): DebertaV2Attention(\n",
       "            (self): DisentangledSelfAttention(\n",
       "              (query_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (pos_dropout): StableDropout()\n",
       "              (dropout): StableDropout()\n",
       "            )\n",
       "            (output): DebertaV2SelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-07, elementwise_affine=True)\n",
       "              (dropout): StableDropout()\n",
       "            )\n",
       "          )\n",
       "          (intermediate): DebertaV2Intermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): DebertaV2Output(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-07, elementwise_affine=True)\n",
       "            (dropout): StableDropout()\n",
       "          )\n",
       "        )\n",
       "        (15): DebertaV2Layer(\n",
       "          (attention): DebertaV2Attention(\n",
       "            (self): DisentangledSelfAttention(\n",
       "              (query_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (pos_dropout): StableDropout()\n",
       "              (dropout): StableDropout()\n",
       "            )\n",
       "            (output): DebertaV2SelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-07, elementwise_affine=True)\n",
       "              (dropout): StableDropout()\n",
       "            )\n",
       "          )\n",
       "          (intermediate): DebertaV2Intermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): DebertaV2Output(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-07, elementwise_affine=True)\n",
       "            (dropout): StableDropout()\n",
       "          )\n",
       "        )\n",
       "        (16): DebertaV2Layer(\n",
       "          (attention): DebertaV2Attention(\n",
       "            (self): DisentangledSelfAttention(\n",
       "              (query_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (pos_dropout): StableDropout()\n",
       "              (dropout): StableDropout()\n",
       "            )\n",
       "            (output): DebertaV2SelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-07, elementwise_affine=True)\n",
       "              (dropout): StableDropout()\n",
       "            )\n",
       "          )\n",
       "          (intermediate): DebertaV2Intermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): DebertaV2Output(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-07, elementwise_affine=True)\n",
       "            (dropout): StableDropout()\n",
       "          )\n",
       "        )\n",
       "        (17): DebertaV2Layer(\n",
       "          (attention): DebertaV2Attention(\n",
       "            (self): DisentangledSelfAttention(\n",
       "              (query_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (pos_dropout): StableDropout()\n",
       "              (dropout): StableDropout()\n",
       "            )\n",
       "            (output): DebertaV2SelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-07, elementwise_affine=True)\n",
       "              (dropout): StableDropout()\n",
       "            )\n",
       "          )\n",
       "          (intermediate): DebertaV2Intermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): DebertaV2Output(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-07, elementwise_affine=True)\n",
       "            (dropout): StableDropout()\n",
       "          )\n",
       "        )\n",
       "        (18): DebertaV2Layer(\n",
       "          (attention): DebertaV2Attention(\n",
       "            (self): DisentangledSelfAttention(\n",
       "              (query_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (pos_dropout): StableDropout()\n",
       "              (dropout): StableDropout()\n",
       "            )\n",
       "            (output): DebertaV2SelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-07, elementwise_affine=True)\n",
       "              (dropout): StableDropout()\n",
       "            )\n",
       "          )\n",
       "          (intermediate): DebertaV2Intermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): DebertaV2Output(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-07, elementwise_affine=True)\n",
       "            (dropout): StableDropout()\n",
       "          )\n",
       "        )\n",
       "        (19): DebertaV2Layer(\n",
       "          (attention): DebertaV2Attention(\n",
       "            (self): DisentangledSelfAttention(\n",
       "              (query_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (pos_dropout): StableDropout()\n",
       "              (dropout): StableDropout()\n",
       "            )\n",
       "            (output): DebertaV2SelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-07, elementwise_affine=True)\n",
       "              (dropout): StableDropout()\n",
       "            )\n",
       "          )\n",
       "          (intermediate): DebertaV2Intermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): DebertaV2Output(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-07, elementwise_affine=True)\n",
       "            (dropout): StableDropout()\n",
       "          )\n",
       "        )\n",
       "        (20): DebertaV2Layer(\n",
       "          (attention): DebertaV2Attention(\n",
       "            (self): DisentangledSelfAttention(\n",
       "              (query_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (pos_dropout): StableDropout()\n",
       "              (dropout): StableDropout()\n",
       "            )\n",
       "            (output): DebertaV2SelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-07, elementwise_affine=True)\n",
       "              (dropout): StableDropout()\n",
       "            )\n",
       "          )\n",
       "          (intermediate): DebertaV2Intermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): DebertaV2Output(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-07, elementwise_affine=True)\n",
       "            (dropout): StableDropout()\n",
       "          )\n",
       "        )\n",
       "        (21): DebertaV2Layer(\n",
       "          (attention): DebertaV2Attention(\n",
       "            (self): DisentangledSelfAttention(\n",
       "              (query_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (pos_dropout): StableDropout()\n",
       "              (dropout): StableDropout()\n",
       "            )\n",
       "            (output): DebertaV2SelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-07, elementwise_affine=True)\n",
       "              (dropout): StableDropout()\n",
       "            )\n",
       "          )\n",
       "          (intermediate): DebertaV2Intermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): DebertaV2Output(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-07, elementwise_affine=True)\n",
       "            (dropout): StableDropout()\n",
       "          )\n",
       "        )\n",
       "        (22): DebertaV2Layer(\n",
       "          (attention): DebertaV2Attention(\n",
       "            (self): DisentangledSelfAttention(\n",
       "              (query_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (pos_dropout): StableDropout()\n",
       "              (dropout): StableDropout()\n",
       "            )\n",
       "            (output): DebertaV2SelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-07, elementwise_affine=True)\n",
       "              (dropout): StableDropout()\n",
       "            )\n",
       "          )\n",
       "          (intermediate): DebertaV2Intermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): DebertaV2Output(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-07, elementwise_affine=True)\n",
       "            (dropout): StableDropout()\n",
       "          )\n",
       "        )\n",
       "        (23): DebertaV2Layer(\n",
       "          (attention): DebertaV2Attention(\n",
       "            (self): DisentangledSelfAttention(\n",
       "              (query_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (pos_dropout): StableDropout()\n",
       "              (dropout): StableDropout()\n",
       "            )\n",
       "            (output): DebertaV2SelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-07, elementwise_affine=True)\n",
       "              (dropout): StableDropout()\n",
       "            )\n",
       "          )\n",
       "          (intermediate): DebertaV2Intermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): DebertaV2Output(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-07, elementwise_affine=True)\n",
       "            (dropout): StableDropout()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (rel_embeddings): Embedding(512, 1024)\n",
       "      (LayerNorm): LayerNorm((1024,), eps=1e-07, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (pooler): ContextPooler(\n",
       "    (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "    (dropout): StableDropout()\n",
       "  )\n",
       "  (classifier): Linear(in_features=1024, out_features=1, bias=True)\n",
       "  (dropout): StableDropout()\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = \"cuda:0\"\n",
    "model_path = \"microsoft/deberta-v3-large\"\n",
    "model = AutoModelForMultipleChoice.from_pretrained(model_path, cache_dir=\"./cache\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True, cache_dir=\"./cache\")  \n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "128100"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.config.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "old_tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True, cache_dir=\"./cache\")  \n",
    "\n",
    "def get_training_corpus():\n",
    "    dataset = eng_dataset[\"train\"]\n",
    "    for start_idx in range(0, len(dataset), 200):\n",
    "        samples = dataset[start_idx : start_idx + 200]\n",
    "        yield \" \".join([\"\".join([samples[col][i] for col in [\"question\", \"explanation\", \"A\", \"B\", \"C\", \"D\"]])\n",
    "                        for i in range(len(samples) - 1)])\n",
    "\n",
    "training_corpus = get_training_corpus()\n",
    "my_tokenizer = tokenizer.train_new_from_iterator(training_corpus, 140000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(131442, 1024)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_tokens = set(my_tokenizer.vocab.keys()) - set(tokenizer.vocab.keys())\n",
    "\n",
    "# add the tokens to the tokenizer vocabulary\n",
    "tokenizer.add_tokens(list(my_tokenizer.vocab))\n",
    "\n",
    "# add new, random embeddings for the new tokens\n",
    "model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LENGTH OF CONTEXT PLUS QUESTION ANSWER\n",
    "MAX_INPUT = 256\n",
    "option_to_index = {option: idx for idx, option in enumerate('ABCD')}\n",
    "index_to_option = {v: k for k,v in option_to_index.items()}\n",
    "def preprocess(example):\n",
    "    # nums = len(example[\"choices\"])\n",
    "    nums = 4\n",
    "    choices = \"ABCD\" if nums == 4 else \"ABC\"\n",
    "    explain = \"\" if example['explanation'] is None else example['explanation']\n",
    "    first_sentence = [ \"[CLS] \" + explain] * nums\n",
    "    second_sentences = [\" #### \" + example['question'] + \" [SEP] \" + example[option] + \" [SEP]\" for option in choices]\n",
    "    tokenized_example = tokenizer(first_sentence, second_sentences, \n",
    "                                  truncation='only_first' \n",
    "                                  if len(second_sentences[0]) < len(first_sentence[0]) else \"only_second\", \n",
    "                                  max_length=MAX_INPUT, add_special_tokens=False, padding=\"max_length\")\n",
    "    tokenized_example['label'] = option_to_index[example['clean_answer']]\n",
    "    \n",
    "    return tokenized_example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'Every grocery store offers two types of water, small bottles of 0.45 liters, large bottles of 0.75 liters. The store offers 20 small bottles and 15 large bottles. The store has provided the number of liters of water:',\n",
       " 'choices': ['A. 20,25 lít', 'B. 19,75 lít', 'C. 21,75 lít', 'D. 22,15 lít'],\n",
       " 'explanation': \"It's not like we're going to have to do this.\",\n",
       " 'answer': 'A. 20,25 lít',\n",
       " 'clean_choices': ['20,25 lít', '19,75 lít', '21,75 lít', '22,15 lít'],\n",
       " 'clean_answer': 'A',\n",
       " 'A': '20.25 liters',\n",
       " 'B': '19.75 liters',\n",
       " 'C': '21.75 liters',\n",
       " 'D': '22.15 liters',\n",
       " 'id': '890'}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eng_dataset[\"train\"][677]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 38798/38798 [02:10<00:00, 297.80 examples/s]\n",
      "Map: 100%|██████████| 120/120 [00:00<00:00, 216.90 examples/s]\n"
     ]
    }
   ],
   "source": [
    "tokenized_dataset = eng_dataset.map(preprocess, remove_columns=eng_dataset[\"train\"].column_names, batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from peft import   (get_peft_config, \n",
    "#                     get_peft_model, \n",
    "#                     PromptTuningInit, \n",
    "#                     PromptTuningConfig, \n",
    "#                     TaskType, \n",
    "#                     PeftType, \n",
    "#                     LoraConfig)\n",
    "\n",
    "# peft_config = LoraConfig(\n",
    "#     r=8, lora_alpha=4, task_type=TaskType.SEQ_CLS, lora_dropout=0.1, \n",
    "#     bias=\"none\", inference_mode=False, \n",
    "#     target_modules=[\"query_proj\", \"value_proj\"],\n",
    "#     modules_to_save=['classifier','pooler'],\n",
    "# )\n",
    "# model = get_peft_model(model, peft_config)\n",
    "# model.print_trainable_parameters()\n",
    "\n",
    "# # DEBERTA LARGE HAS TOTAL OF 24 LAYERS\n",
    "# FREEZE_LAYERS = 18\n",
    "# # BOOLEAN TO FREEZE EMBEDDINGS\n",
    "# FREEZE_EMBEDDINGS = True\n",
    "\n",
    "# if FREEZE_EMBEDDINGS:\n",
    "#     print('Freezing embeddings.')\n",
    "#     for param in model.deberta.embeddings.parameters():\n",
    "#         param.requires_grad = False\n",
    "# if FREEZE_LAYERS>0:\n",
    "#     print(f'Freezing {FREEZE_LAYERS} layers.')\n",
    "#     for layer in model.deberta.encoder.layer[:FREEZE_LAYERS]:\n",
    "#         for param in layer.parameters():\n",
    "#             param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(model.deberta.encoder.layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def map_at_3(predictions, labels):\n",
    "    map_sum = 0\n",
    "    pred = np.argsort(-1*np.array(predictions),axis=1)[:,:3]\n",
    "    for x,y in zip(pred,labels):\n",
    "        z = [1/i if y==j else 0 for i,j in zip([1,2,3],x)]\n",
    "        map_sum += np.sum(z)\n",
    "    return map_sum / len(predictions)\n",
    "\n",
    "def compute_metrics(p):\n",
    "    predictions = p.predictions\n",
    "    labels = p.label_ids\n",
    "    return {\n",
    "        \"accuracy\": sum(predictions.argmax(axis=1) == labels) / len(labels), \n",
    "        \"map@3\": map_at_3(predictions.tolist(), labels.tolist())}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "VER = 5\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    warmup_ratio=0.1, \n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=2,\n",
    "    num_train_epochs=5,\n",
    "    report_to='none',\n",
    "    output_dir = f'./checkpoints/checkpoints_{VER}',\n",
    "    overwrite_output_dir=True,\n",
    "    # fp16=True,\n",
    "    gradient_accumulation_steps=8,\n",
    "    logging_steps=25,\n",
    "    evaluation_strategy='steps',\n",
    "    eval_steps=25,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=25,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model='map@3',\n",
    "    lr_scheduler_type='cosine',\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from transformers.tokenization_utils_base import PreTrainedTokenizerBase, PaddingStrategy\n",
    "from typing import Optional, Union\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class DataCollatorForMultipleChoice:\n",
    "    tokenizer: PreTrainedTokenizerBase\n",
    "    padding: Union[bool, str, PaddingStrategy] = True\n",
    "    max_length: Optional[int] = None\n",
    "    pad_to_multiple_of: Optional[int] = None\n",
    "    \n",
    "    def __call__(self, features):\n",
    "        label_name = 'label' if 'label' in features[0].keys() else 'labels'\n",
    "        labels = [feature.pop(label_name) for feature in features]\n",
    "        batch_size = len(features)\n",
    "        num_choices = len(features[0]['input_ids'])\n",
    "        flattened_features = [\n",
    "            [{k: v[i] for k, v in feature.items()} for i in range(num_choices)] for feature in features\n",
    "        ]\n",
    "        flattened_features = sum(flattened_features, [])\n",
    "        \n",
    "        batch = self.tokenizer.pad(\n",
    "            flattened_features,\n",
    "            padding=self.padding,\n",
    "            max_length=self.max_length,\n",
    "            pad_to_multiple_of=self.pad_to_multiple_of,\n",
    "            return_tensors='pt',\n",
    "        )\n",
    "        batch = {k: v.view(batch_size, num_choices, -1) for k, v in batch.items()}\n",
    "        batch['labels'] = torch.tensor(labels, dtype=torch.int64)\n",
    "        return batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "/space/hotel/phit/miniconda3/envs/zalo/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4' max='3030' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [   4/3030 00:13 < 5:37:51, 0.15 it/s, Epoch 0.00/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/space/hotel/phit/miniconda3/envs/zalo/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/space/hotel/phit/miniconda3/envs/zalo/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/space/hotel/phit/miniconda3/envs/zalo/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/space/hotel/phit/miniconda3/envs/zalo/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/space/hotel/phit/miniconda3/envs/zalo/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/space/hotel/phit/miniconda3/envs/zalo/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/space/hotel/phit/miniconda3/envs/zalo/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/space/hotel/phit/miniconda3/envs/zalo/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/space/hotel/phit/miniconda3/envs/zalo/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    }
   ],
   "source": [
    "from transformers import EarlyStoppingCallback\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=DataCollatorForMultipleChoice(tokenizer=tokenizer),\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"test\"],\n",
    "    compute_metrics = compute_metrics,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=6)],\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "trainer.save_model(f'./best_model/model_v{VER}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/space/hotel/phit/miniconda3/envs/zalo/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "test_predictions = trainer.predict(tokenized_dataset[\"test\"]).predictions\n",
    "predictions_as_ids = np.argsort(-test_predictions, 1)\n",
    "predictions_as_answer_letters = np.array(list('ABCD'))[predictions_as_ids]\n",
    "# predictions_as_string = test_df['prediction'] = [\n",
    "#     ' '.join(row) for row in predictions_as_answer_letters[:, :3]\n",
    "# ]\n",
    "\n",
    "sum([index_to_option[i] for i in tokenized_dataset[\"test\"][\"label\"]] == predictions_as_answer_letters[:, :1].squeeze())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "120"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenized_dataset[\"test\"][\"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "test_predictions = trainer.predict(tokenized_dataset[\"test\"]).predictions\n",
    "predictions_as_ids = np.argsort(-test_predictions, 1)\n",
    "predictions_as_answer_letters = np.array(list('ABCD'))[predictions_as_ids]\n",
    "\n",
    "sum([index_to_option[i] for i in tokenized_dataset[\"test\"][\"label\"]] == predictions_as_answer_letters[:, :1].squeeze())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 189/189 [00:19<00:00,  9.72 examples/s]\n"
     ]
    }
   ],
   "source": [
    "from transformers import ( \n",
    "                        AutoTokenizer,\n",
    "                        AutoModelForSeq2SeqLM)\n",
    "import torch\n",
    "import re\n",
    "import json \n",
    "\n",
    "test = json.loads(open('data/math_test.json').read())[\"data\"]\n",
    "test_df = pd.DataFrame(test)\n",
    "test_df.set_index(\"id\", inplace=True)\n",
    "\n",
    "pattern = re.compile(\"[ABCD].\")\n",
    "\n",
    "def process(texts):\n",
    "    return [pattern.split(text)[-1].strip() for text in texts]\n",
    "test_df[\"clean_choices\"] = test_df[\"choices\"].apply(process)\n",
    "\n",
    "choices = {choice: i for i, choice in enumerate(\"ABCD\")} \n",
    "\n",
    "def make_choice(df, choice):\n",
    "    idx = choices[choice]\n",
    "    df[choice] = df[\"clean_choices\"].apply(lambda x: x[idx] if idx < len(x) else \"\")\n",
    "    return df\n",
    "for choice in choices.keys():\n",
    "    make_choice(test_df, choice)\n",
    "\n",
    "test_dataset = Dataset.from_pandas(test_df)\n",
    "\n",
    "\n",
    "tokenizer_vi2en = AutoTokenizer.from_pretrained(\"vinai/vinai-translate-vi2en-v2\", src_lang=\"vi_VN\", cache_dir=\"./cache\")\n",
    "model_vi2en = AutoModelForSeq2SeqLM.from_pretrained(\"vinai/vinai-translate-vi2en-v2\", cache_dir=\"./cache\")\n",
    "device_vi2en = torch.device(\"cuda\")\n",
    "model_vi2en.to(device_vi2en)\n",
    "\n",
    "def translate_vi2en(example) -> str:\n",
    "    for col in [\"question\", \"A\", \"B\", \"C\", \"D\"]:\n",
    "        input_ids = tokenizer_vi2en(example[col], padding=True, return_tensors=\"pt\").to(device_vi2en)\n",
    "        output_ids = model_vi2en.generate(\n",
    "            **input_ids,\n",
    "            decoder_start_token_id=tokenizer_vi2en.lang_code_to_id[\"en_XX\"],\n",
    "            num_return_sequences=1,\n",
    "            num_beams=5,\n",
    "            early_stopping=True\n",
    "        )\n",
    "        example[col] = tokenizer_vi2en.batch_decode(output_ids, skip_special_tokens=True)\n",
    "        # Free GPU memory\n",
    "        del input_ids\n",
    "        del output_ids\n",
    "        torch.cuda.empty_cache()\n",
    "    return example\n",
    "\n",
    "test_dataset = test_dataset.map(translate_vi2en, batched=True, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 189/189 [00:00<00:00, 546.51 examples/s]\n"
     ]
    }
   ],
   "source": [
    "def preprocess(example):\n",
    "    # nums = len(example[\"choices\"])\n",
    "    nums = 4\n",
    "    choices = \"ABCD\" if nums == 4 else \"ABC\"\n",
    "    first_sentence = [example['question']] * nums\n",
    "    second_sentences = [example[option] for option in choices]\n",
    "    tokenized_example = tokenizer(first_sentence, second_sentences, \n",
    "                                  truncation=True,max_length=MAX_INPUT, padding=\"max_length\") \n",
    "                                #   if len(second_sentences[0]) < len(first_sentence[0]) else \"only_second\", \n",
    "                                #   max_length=MAX_INPUT, add_special_tokens=False, padding=\"max_length\")\n",
    "    # tokenized_example['label'] = option_to_index[example['clean_answer']]\n",
    "    \n",
    "    return tokenized_example\n",
    "\n",
    "tokenized_test_dataset = test_dataset.map(preprocess, remove_columns=test_dataset.column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'token_type_ids', 'attention_mask'],\n",
       "    num_rows: 189\n",
       "})"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='10' max='12' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [10/12 00:04 < 00:01, 1.89 it/s]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "test_predictions = trainer.predict(tokenized_test_dataset).predictions\n",
    "predictions_as_ids = np.argsort(-test_predictions, 1)\n",
    "predictions_as_answer_letters = np.array(list('ABCD'))[predictions_as_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>choices</th>\n",
       "      <th>clean_choices</th>\n",
       "      <th>A</th>\n",
       "      <th>B</th>\n",
       "      <th>C</th>\n",
       "      <th>D</th>\n",
       "      <th>id_ans</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>01-0203</th>\n",
       "      <td>Một cửa hàng đã bán 30% số hàng hiện có và thu...</td>\n",
       "      <td>[A. 4 500 000 đồng, B. 45 000 000 đồng, C. 50 ...</td>\n",
       "      <td>[4 500 000 đồng, 45 000 000 đồng, 50 000 000 đ...</td>\n",
       "      <td>4 500 000 đồng</td>\n",
       "      <td>45 000 000 đồng</td>\n",
       "      <td>50 000 000 đồng</td>\n",
       "      <td>450 000 000 đồng</td>\n",
       "      <td>[0, 3, 1, 2]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>01-0206</th>\n",
       "      <td>Một người đi xe đạp từ A lúc 7 giờ với vận tốc...</td>\n",
       "      <td>[A. 24 phút, B. 1 giờ, C. 7 giờ 24 phút, D. 8 ...</td>\n",
       "      <td>[24 phút, 1 giờ, 7 giờ 24 phút, 8 giờ 24 phút]</td>\n",
       "      <td>24 phút</td>\n",
       "      <td>1 giờ</td>\n",
       "      <td>7 giờ 24 phút</td>\n",
       "      <td>8 giờ 24 phút</td>\n",
       "      <td>[1, 3, 2, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>01-0207</th>\n",
       "      <td>Cạnh của hình lập phương gấp lên 2 lần thì diệ...</td>\n",
       "      <td>[A. 2 lần, B. 4 lần, C. 6 lần, D. 8 lần]</td>\n",
       "      <td>[2 lần, 4 lần, 6 lần, 8 lần]</td>\n",
       "      <td>2 lần</td>\n",
       "      <td>4 lần</td>\n",
       "      <td>6 lần</td>\n",
       "      <td>8 lần</td>\n",
       "      <td>[1, 2, 3, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>01-0209</th>\n",
       "      <td>Một thửa ruộng hình thang có đáy bé dài 8m, đá...</td>\n",
       "      <td>[A. 125m^{2}, B. 20%, C. 25%, D. 50%]</td>\n",
       "      <td>[125m^{2}, 20%, 25%, 50%]</td>\n",
       "      <td>125m^{2}</td>\n",
       "      <td>20%</td>\n",
       "      <td>25%</td>\n",
       "      <td>50%</td>\n",
       "      <td>[2, 3, 1, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>01-0210</th>\n",
       "      <td>Một xe lửa vượt qua cái cầu dài 450m hết 45 gi...</td>\n",
       "      <td>[A. 3m, B. 200m, C. 200m, D. 225m]</td>\n",
       "      <td>[3m, 200m, 200m, 225m]</td>\n",
       "      <td>3m</td>\n",
       "      <td>200m</td>\n",
       "      <td>200m</td>\n",
       "      <td>225m</td>\n",
       "      <td>[3, 1, 2, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>01-0698</th>\n",
       "      <td>Kết quả phép nhân 4,51 \\times 10 là:</td>\n",
       "      <td>[A. 451, B. 4,51, C. 45,1, D. 45]</td>\n",
       "      <td>[451, 4,51, 45,1, 45]</td>\n",
       "      <td>451</td>\n",
       "      <td>4,51</td>\n",
       "      <td>45,1</td>\n",
       "      <td>45</td>\n",
       "      <td>[3, 1, 2, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>01-0703</th>\n",
       "      <td>Lớp 5/2 có 32 học sinh, trong đó có 12 học sin...</td>\n",
       "      <td>[A. 375 %, B. 37,5 %, C. 3,75 %, D. 0,375 %]</td>\n",
       "      <td>[375 %, 37,5 %, 3,75 %, 0,375 %]</td>\n",
       "      <td>375 %</td>\n",
       "      <td>37,5 %</td>\n",
       "      <td>3,75 %</td>\n",
       "      <td>0,375 %</td>\n",
       "      <td>[1, 2, 0, 3]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>01-0715</th>\n",
       "      <td>Số thập phân thích hợp để điền vào chỗ chấm: 4...</td>\n",
       "      <td>[A. 45,62, B. 4,562, C. 456,2, D. 4562]</td>\n",
       "      <td>[45,62, 4,562, 456,2, 4562]</td>\n",
       "      <td>45,62</td>\n",
       "      <td>4,562</td>\n",
       "      <td>456,2</td>\n",
       "      <td>4562</td>\n",
       "      <td>[3, 2, 0, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>01-0716</th>\n",
       "      <td>Kết quả của X trong biểu thức:  X \\div 2,04 = ...</td>\n",
       "      <td>[A. 3,03, B. 3,04, C. 3,05, D. 3,06]</td>\n",
       "      <td>[3,03, 3,04, 3,05, 3,06]</td>\n",
       "      <td>3,03</td>\n",
       "      <td>3,04</td>\n",
       "      <td>3,05</td>\n",
       "      <td>3,06</td>\n",
       "      <td>[2, 3, 1, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>01-0717</th>\n",
       "      <td>Tính diện tích hình tam giác có độ dài đáy là ...</td>\n",
       "      <td>[A. 183 dm^{2}, B. 184 dm^{2}, C. 185 dm^{2}, ...</td>\n",
       "      <td>[183 dm^{2}, 184 dm^{2}, 185 dm^{2}, 186 dm^{2}]</td>\n",
       "      <td>183 dm^{2}</td>\n",
       "      <td>184 dm^{2}</td>\n",
       "      <td>185 dm^{2}</td>\n",
       "      <td>186 dm^{2}</td>\n",
       "      <td>[0, 1, 3, 2]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>189 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  question  \\\n",
       "id                                                           \n",
       "01-0203  Một cửa hàng đã bán 30% số hàng hiện có và thu...   \n",
       "01-0206  Một người đi xe đạp từ A lúc 7 giờ với vận tốc...   \n",
       "01-0207  Cạnh của hình lập phương gấp lên 2 lần thì diệ...   \n",
       "01-0209  Một thửa ruộng hình thang có đáy bé dài 8m, đá...   \n",
       "01-0210  Một xe lửa vượt qua cái cầu dài 450m hết 45 gi...   \n",
       "...                                                    ...   \n",
       "01-0698               Kết quả phép nhân 4,51 \\times 10 là:   \n",
       "01-0703  Lớp 5/2 có 32 học sinh, trong đó có 12 học sin...   \n",
       "01-0715  Số thập phân thích hợp để điền vào chỗ chấm: 4...   \n",
       "01-0716  Kết quả của X trong biểu thức:  X \\div 2,04 = ...   \n",
       "01-0717  Tính diện tích hình tam giác có độ dài đáy là ...   \n",
       "\n",
       "                                                   choices  \\\n",
       "id                                                           \n",
       "01-0203  [A. 4 500 000 đồng, B. 45 000 000 đồng, C. 50 ...   \n",
       "01-0206  [A. 24 phút, B. 1 giờ, C. 7 giờ 24 phút, D. 8 ...   \n",
       "01-0207           [A. 2 lần, B. 4 lần, C. 6 lần, D. 8 lần]   \n",
       "01-0209              [A. 125m^{2}, B. 20%, C. 25%, D. 50%]   \n",
       "01-0210                 [A. 3m, B. 200m, C. 200m, D. 225m]   \n",
       "...                                                    ...   \n",
       "01-0698                  [A. 451, B. 4,51, C. 45,1, D. 45]   \n",
       "01-0703       [A. 375 %, B. 37,5 %, C. 3,75 %, D. 0,375 %]   \n",
       "01-0715            [A. 45,62, B. 4,562, C. 456,2, D. 4562]   \n",
       "01-0716               [A. 3,03, B. 3,04, C. 3,05, D. 3,06]   \n",
       "01-0717  [A. 183 dm^{2}, B. 184 dm^{2}, C. 185 dm^{2}, ...   \n",
       "\n",
       "                                             clean_choices               A  \\\n",
       "id                                                                           \n",
       "01-0203  [4 500 000 đồng, 45 000 000 đồng, 50 000 000 đ...  4 500 000 đồng   \n",
       "01-0206     [24 phút, 1 giờ, 7 giờ 24 phút, 8 giờ 24 phút]         24 phút   \n",
       "01-0207                       [2 lần, 4 lần, 6 lần, 8 lần]           2 lần   \n",
       "01-0209                          [125m^{2}, 20%, 25%, 50%]        125m^{2}   \n",
       "01-0210                             [3m, 200m, 200m, 225m]              3m   \n",
       "...                                                    ...             ...   \n",
       "01-0698                              [451, 4,51, 45,1, 45]             451   \n",
       "01-0703                   [375 %, 37,5 %, 3,75 %, 0,375 %]           375 %   \n",
       "01-0715                        [45,62, 4,562, 456,2, 4562]           45,62   \n",
       "01-0716                           [3,03, 3,04, 3,05, 3,06]            3,03   \n",
       "01-0717   [183 dm^{2}, 184 dm^{2}, 185 dm^{2}, 186 dm^{2}]      183 dm^{2}   \n",
       "\n",
       "                       B                C                 D        id_ans  \n",
       "id                                                                         \n",
       "01-0203  45 000 000 đồng  50 000 000 đồng  450 000 000 đồng  [0, 3, 1, 2]  \n",
       "01-0206            1 giờ    7 giờ 24 phút     8 giờ 24 phút  [1, 3, 2, 0]  \n",
       "01-0207            4 lần            6 lần             8 lần  [1, 2, 3, 0]  \n",
       "01-0209              20%              25%               50%  [2, 3, 1, 0]  \n",
       "01-0210             200m             200m              225m  [3, 1, 2, 0]  \n",
       "...                  ...              ...               ...           ...  \n",
       "01-0698             4,51             45,1                45  [3, 1, 2, 0]  \n",
       "01-0703           37,5 %           3,75 %           0,375 %  [1, 2, 0, 3]  \n",
       "01-0715            4,562            456,2              4562  [3, 2, 0, 1]  \n",
       "01-0716             3,04             3,05              3,06  [2, 3, 1, 0]  \n",
       "01-0717       184 dm^{2}       185 dm^{2}        186 dm^{2}  [0, 1, 3, 2]  \n",
       "\n",
       "[189 rows x 8 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test_df.drop(columns=test_df.columns, inplace=True)\n",
    "test_df[\"id_ans\"] = predictions_as_ids.squeeze().tolist()\n",
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>answer</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>01-0203</th>\n",
       "      <td>A. 4 500 000 đồng</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>01-0206</th>\n",
       "      <td>B. 1 giờ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>01-0207</th>\n",
       "      <td>B. 4 lần</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>01-0209</th>\n",
       "      <td>C. 25%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>01-0210</th>\n",
       "      <td>D. 225m</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>01-0698</th>\n",
       "      <td>D. 45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>01-0703</th>\n",
       "      <td>B. 37,5 %</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>01-0715</th>\n",
       "      <td>D. 4562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>01-0716</th>\n",
       "      <td>C. 3,05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>01-0717</th>\n",
       "      <td>A. 183 dm^{2}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>189 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    answer\n",
       "id                        \n",
       "01-0203  A. 4 500 000 đồng\n",
       "01-0206           B. 1 giờ\n",
       "01-0207           B. 4 lần\n",
       "01-0209             C. 25%\n",
       "01-0210            D. 225m\n",
       "...                    ...\n",
       "01-0698              D. 45\n",
       "01-0703          B. 37,5 %\n",
       "01-0715            D. 4562\n",
       "01-0716            C. 3,05\n",
       "01-0717      A. 183 dm^{2}\n",
       "\n",
       "[189 rows x 1 columns]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df[\"answer\"] = test_df.apply(lambda x: [x[\"choices\"][int(i)] for i in x[\"id_ans\"] if int(i) < len(x[\"choices\"])][0], axis=1)\n",
    "test_df.drop(columns=[\"question\", \"choices\", \"clean_choices\", \"A\", \"B\", \"C\", \"D\", \"id_ans\"], inplace=True)\n",
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.to_csv(f\"./submissions/submission_{VER}.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = AutoModelForMultipleChoice.from_pretrained(model_path)\n",
    "# model = get_peft_model(model, peft_config)\n",
    "# checkpoint = torch.load(f'model_v{VER}/pytorch_model.bin')\n",
    "# model.load_state_dict(checkpoint)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "zalo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
